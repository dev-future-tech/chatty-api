= USAGE

:author: Anthony Ikeda <anthony.ikeda@gmail.com>

== Local LLM tooling

When using this API with local tool LLM tool LM Studio, you will need to address the issues that LM Studio has with HTTP connections.

There seems to be issues with how LM Studio handles connections, so calling the endpoints (http://127.0.0.1:1234) will not be possible with Spring AI.

=== Running the NGINX Proxy

Once you have installed NGINX the quick way to incorporate the proxy is by updating the `nginx.conf` file. If you installed NGINX using `homebrew` then this is most likely at: `/opt/homebrew/etc/nginx/nginx.conf`.

First, you will want to add the upstream to your config. Position this in the `http` block:

[source,text,numbered]
----
http {
    upstream backend {
        server 127.0.0.1:1234;
    }
}
----

After this we want to enable `http2` on the `server` directive:

[source,text,numbered]
----
http {
    server {
        listen 8090;
        http2 on;
    }
}
----

Next we want to update the `/` route to proxy requests to the backend, LM Studio server:

[source,text,numbered]
----
http {
    server {
        listen 8090;
        http2 on;

        location / {
            proxy_pass http://backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto http;
        }
    }
}
----


=== Set up your Spring Boot Project for LM Studio

Now that NGINX is running and proxying to the LM Studio server, you now need to configure your Spring Boot application.

In your `application.properties` file set up the following properties:

[source,properties,numbered]
----
spring.ai.openai.api-key=sk-proj
spring.ai.openai.base-url=http://localhost:8090
spring.ai.openai.chat.options.model=meta-llama-3.1-8b-instruct
spring.ai.openai.chat.options.temperature=0.7
----

Things to note here:

. Something, basically anything, has to be supplied for the `spring.ai.openai.api-key` since Spring AI will check that a key exists regardless of the LLM provider you use.
. You will use the URL you configured in NGINX (in this case `localhost` on port `8090`) for the `spring.ai.openai.base-url`


== Containerizing

=== Compile the image

[source,bash]
----
$ ./mvnw -Pnative spring-boot:build-image
----


=== Run the Container

[source,bash]
----
$ docker run -p 8070:8070 -p 8081:8081 --mount type=bind,src=./logs,dst=/var/logs -eLOG_DIR=/var/logs chatty-api:0.0.1-SNAPSHOT
----

=== Docker Compose

[source,yaml]
----
name: chatty-api

services:
  database:
    image: postgres:17
    restart: always
    shm_size: 128mb
    volumes:
      - db-data:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: booking_administrator
      POSTGRES_PASSWORD: letmein
      POSTGRES_DB: bookings_db
      PGDATA: /var/lib/postgresql/data/pgdata

  api:
    image: chatty-api:0.0.1-SNAPSHOT
    volumes:
      - ./logs:/var/logs
    environment:
      LOG_DIR: /var/logs
      SPRING_DATASOURCE_URL: jdbc:postgresql://database:5432/bookings_db
      SPRING_LIQUIBASE_DRIVERCLASSNAME: org.postgresql.Driver
      SPRING_LIQUIBASE_PASSWORD: letmein
      SPRING_LIQUIBASE_URL: jdbc:postgresql://database:5432/bookings_db
      SPRING_LIQUIBASE_USER: booking_controller
      SPRING_LIQUIBASE_CHANGELOG: classpath:/db/databaseChangelog.yaml

volumes:
  db-data:
----